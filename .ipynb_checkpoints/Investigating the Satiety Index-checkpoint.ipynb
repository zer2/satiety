{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2e1426",
   "metadata": {},
   "source": [
    "# Calculating the Satiety Index\n",
    "\n",
    "One of the most important concepts in nutrition is the satiety index- the ratio between how filling a food is vs. how many calories it contains. People who eat satiating foods tend to consume fewer calories overall, helping them lose weight. Understanding satiety is also important for gaining weight, since eating un-filling foods allows for the consumption of excess calories \n",
    "\n",
    "Unfortunately, the satiety index is difficult to measure. [Some work](https://www.researchgate.net/publication/15701207_A_Satiety_Index_of_common_foods) has been done to measure the satiety index of common foods, but no comprehensive research exists. The closest I have found is [this article](https://optimisingnutrition.com/calculating-satiety/) which uses a public dataset to investigate the satiation of macronutrients.\n",
    "\n",
    "If we are interested in how satiating various foods are, we can take a similar approach to the aforementioned article, except look at individual foods instead of macronutrients. We will use the same dataset from kaggle, which has records of 10k people's eating habits plus daily calorie goals. We will create a sparse (day record x food consumed) matrix, and use linear regression to estimate how strongly each food contributed to achieving the corresponding calorie goals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "03959d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from scipy.sparse.linalg import lsqr\n",
    "from itertools import compress\n",
    "from operator import itemgetter\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rake_nltk import Rake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e764a",
   "metadata": {},
   "source": [
    "# Part 1: Reading the data and transforming it into df format\n",
    "\n",
    "The data's raw form is a tab-seperated text file with json-like entries for nutrition logs. We are interested only in the names of foods consumed and their calorie content, so we can transform the json logs into series mapping foods to calories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "74118531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:25.172914\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/mfp-diaries.tsv', sep = '\\t', header = None)\n",
    "df.columns = ['PERSON_ID','DATE','NUTRITION','GOALS']\n",
    "\n",
    "def get_float_value(x):\n",
    "    return int(re.sub( ',','',x)) #it is important to store vars as ints instead of strs, to conserve memory later \n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "goal_calories = [json.loads(x['GOALS'])['total'][0]['value'] for k,x in df.iterrows()]\n",
    "\n",
    "start = datetime.now()\n",
    "calorie_records = [pd.Series(\n",
    "                                {y['name'] : get_float_value(y['nutritions'][0]['value'])\n",
    "                                 for y in flatten([z['dishes'] for z in json.loads(x['NUTRITION'])])\n",
    "                                }\n",
    "                            )\n",
    "                for k, x in df.iterrows()]\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451488ba",
   "metadata": {},
   "source": [
    "# Part 2: Creating a sparse matrix from the records \n",
    "\n",
    "Our goal is to have a dataframe with a row for each daily journal and a column for each food consumed, but that is infeasible. There are ~600k daily entries, and ~1M unique foods listed. While the vast majority of cell contents would be simply zero, the resulting dense dataframe would still be far too large to hold in traditional memory. \n",
    "\n",
    "Fortunately, we can use a sparse representation to have a dataframe-esque object without explicitly writing every zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "2d107de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:00.643453\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "all_foods = set().union(*[list(c.index) for c in calorie_records])\n",
    "tot_foods = len(all_foods)\n",
    "mapping = {food : i for food, i in zip(all_foods,range(tot_foods))}\n",
    "sparse_records = [csr_matrix((calorie_record.values,\n",
    "                               ([0] * len(calorie_record)\n",
    "                                ,[mapping[z] for z in calorie_record.index]\n",
    "                               )\n",
    "                              )\n",
    "                             ,shape = (1, tot_foods)\n",
    "                            ) for calorie_record in calorie_records]\n",
    "sparse_matrix = vstack(sparse_records)\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161392b7",
   "metadata": {},
   "source": [
    "# Part 3: Calculating inferred satiety indexes by food\n",
    "\n",
    "We overcame the technical hurdle of holding the dataset in memory, but there is also a mathematical hurdle, which is that we have more columns than rows. We don't want to suffer from the curse of dimensionality so we need to reduce our features somehow. \n",
    "\n",
    "One approach is to lump all of the rare foods together. This will add some noise to the model, but it will also allow the model to focus on the more common foods, and hopefully come up with solid estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "d120a916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:07:38.607748\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "food_counts = sparse_matrix.sum(axis = 0)\n",
    "food_count_significant = [f >= 1000 for f in food_counts] #we will lump all the 'insignificant' foods together\n",
    "significant_food_matrix = sparse_matrix[:, food_count_significant[0].tolist()[0]]\n",
    "insignificant_food_matrix = sparse_matrix[:, (~food_count_significant[0]).tolist()[0]]\n",
    "\n",
    "insignificant_food_calories = insignificant_food_matrix.sum(axis = 1)\n",
    "\n",
    "adjusted_significant_food_matrix = hstack([significant_food_matrix, insignificant_food_calories])\n",
    "res = lsqr(adjusted_significant_food_matrix\n",
    "           , goal_calories\n",
    "           , x0 = [1] * adjusted_significant_food_matrix.shape[1])\n",
    "\n",
    "significant_foods = list(compress(list(mapping.keys()),food_count_significant[0].tolist()[0]))\n",
    "significant_food_ratios = pd.Series(res[0], index = significant_foods + ['Other'])\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "d916ed95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fresh - Green Onion, Chopped, 1/4 cup                                                               22.564728\n",
       "Kirkland Signature (Costco) - Extra Strength Glucosamine Hci 1500 Mg With Msm 1500 Mg, 2 Tablets    12.591241\n",
       "Tesco - Organic Spinach, 50 g                                                                        9.197528\n",
       "Generic - Tea With 40ml Whole Milk, 1 Mug                                                            8.924278\n",
       "Generic - Green Beens Boiled, 3 cup (125 grams)                                                      8.692831\n",
       "                                                                                                      ...    \n",
       "Kirkland - Vitamin C 500 Mg Chewable, 2 tablet                                                      -5.455760\n",
       "Nescafe Taster's Choice - Single Serve Packet - Hazelnut Instant Coffee, 2 packet                   -5.476055\n",
       "Generic - Rice, Jasmin, Boiled, 150 g                                                               -6.706598\n",
       "Classic - Tomato, 100 g                                                                             -8.144594\n",
       "2/7 black beans, 0.5 serving(s)                                                                     -8.535910\n",
       "Length: 156904, dtype: float64"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_food_ratios.sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "2433d159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.604063870539085"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_food_matrix.sum()/sparse_matrix.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f84363",
   "metadata": {},
   "source": [
    "Even after reducing our features significantly (from 1M to ~157k), the results still look fairly unreliable. We don't have the option of excluding more foods because we are already cutting out 40% of all calories consumed, and cutting out more than that would add excessive noise. So what other options do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566f93e",
   "metadata": {},
   "source": [
    "# Part 4: Finding satiety index by category\n",
    "\n",
    "One step we could take is categorizing foods by type. \"Potato\" and \"Sweet Potato\" could be mixed together, for example . \n",
    "\n",
    "To do this we will embed the food descriptions with Sentence-Bert, then categorize them using K-means clustering. Using LDA is another option but it is not appropriate because it assigns multiple topics to each input document, which is not what we want. We will also rake-nltk to summarize the categories for more understandability. In an ideal world we would do this analysis for every food, but it would take a long time so we will use the same procedure as before, limiting to only the common foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "7b622b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Zach\n",
      "[nltk_data]     Rosenof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Zach\n",
      "[nltk_data]     Rosenof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zach\n",
      "[nltk_data]     Rosenof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Zach\n",
      "[nltk_data]     Rosenof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "additional_stopwords = {'oz','ozs','cup','cups','small','medium','large','gram','grams','pound','serving','tbsp'\n",
    "                       ,'container','order','serving(s)','tbls','mini','inch','servings','standard','white','black'\n",
    "                       ,'regular','homemade'}\n",
    "full_stopwords = gensim.parsing.preprocessing.STOPWORDS.union(additional_stopwords)\n",
    "\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in full_stopwords and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n",
    "def get_summary_of_cluster(cluster):\n",
    "    text = \". \".join([mapping_inverted[x] for x in cluster_map[cluster]])\n",
    "    rake_nltk_var.extract_keywords_from_text(text)\n",
    "    keyword_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "    return keyword_extracted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "processed_docs = []\n",
    "\n",
    "for doc in significant_foods:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode([z if len(z) > 1 else (z*2 if len(z) == 1 else ['blank','blank']) for z in processed_docs]) \n",
    "\n",
    "km = KMeans(n_clusters = 300)\n",
    "km.fit(embeddings)\n",
    "\n",
    "cluster_map = {}\n",
    "for food, label in zip(significant_foods, km.labels_):\n",
    "    cluster_map[label] = cluster_map.get(label,[]) + [mapping[food]]\n",
    "cluster_list = list(cluster_map.keys())\n",
    "\n",
    "clumped_matrix = np.concatenate([sparse_matrix[:,cluster_map[cluster]].sum(axis = 1) for cluster in cluster_list]\n",
    "                                , axis = 1)\n",
    "clumped_matrix = np.concatenate([clumped_matrix, insignificant_food_calories]\n",
    "                                , axis = 1)\n",
    "\n",
    "res = np.linalg.lstsq(clumped_matrix, goal_calories)    \n",
    "\n",
    "mapping_inverted = {v: k for k, v in mapping.items()}\n",
    "res_final = pd.Series({get_summary_of_cluster(cluster) : res[0][cluster_list.index(cluster)] \n",
    "                       for cluster in cluster_list})\n",
    "\n",
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 300)\n",
    "res_final.sort_values(ascending = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da50ce3",
   "metadata": {},
   "source": [
    "There is some interesting information here. Pure protein products at the top make sense superficially, and could indicate something real. However, the coefficients generally look close to one, suggesting that MyFitnessPal users are largely hitting their calorie targets regardless of what kinds of food they choose to eat. \n",
    "\n",
    "This is not necessarily an indictment on the concept of a satiety index. People who track their own calories are likely capable of adjusting their food intake against what it would be naturally to meet their calorie targets. They might even be mis-reporting their intake, consciously or unconsciously, to keep it in line with their goals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
